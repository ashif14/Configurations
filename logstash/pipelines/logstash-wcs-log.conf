input {
  beats {
    port => 5044
    type => "log"
    id => "filebeat_1"
  }
}

filter {
    multiline{
        # Nothing will pass this filter unless it is a new event ( new [2014-03-02 1.... )
        # multiline adds java error traces to original log entry
        pattern => "^\["
        what => "previous"
        negate=> true
    }
    # If log file is SystemOut
    if [fields][log_type] == "SystemOut" {
        grok {
            # match => ["message", "\[%{DATA:timestamp} %{WORD:tz}\] %{BASE16NUM:was_threadID} %{WORD:was_shortname} %{SPACE}%{WORD:was_loglevel} %{SPACE}%{DATA:message} %{SPACE}REQ_START %{SPACE}%{WORD:transaction_id}"]
            match => ["message", "\[%{DATA:timestamp} %{WORD:tz}\] %{BASE16NUM:was_threadID} %{WORD:was_shortname} %{SPACE}%{WORD:was_loglevel} %{SPACE}%{DATA:message} %{SPACE}REQ_%{[A-Z]:request_status} %{SPACE}%{WORD:transaction_id}"]
            match => ["message", "\[%{DATA:timestamp} %{WORD:tz}\] %{BASE16NUM:was_threadID} %{WORD:was_shortname} %{SPACE}%{WORD:was_loglevel} %{SPACE}%{SPACE}%{SPACE}%{WORD:was_errorcode}: %{SPACE}%{GREEDYDATA:message}"]
            match => ["message", "\[%{DATA:timestamp} %{WORD:tz}\] %{BASE16NUM:was_threadID} %{WORD:was_shortname} %{SPACE}%{WORD:was_loglevel} %{SPACE}%{SPACE}%{SPACE} \[%{GREEDYDATA:was_sibbus}\] +%{WORD:was_errorcode}: %{SPACE}%{GREEDYDATA:message}"]
            match => ["message", "\[%{DATA:timestamp} %{WORD:tz}\] %{BASE16NUM:was_threadID} %{WORD:was_shortname} %{SPACE}%{WORD:was_loglevel} %{GREEDYDATA:message2} +%{WORD:was_errorcode}: %{SPACE}%{GREEDYDATA:message}"]
            match => ["message", "\[%{DATA:timestamp} %{WORD:tz}\] %{BASE16NUM:was_threadID} %{WORD:was_shortname} %{SPACE}%{WORD:was_loglevel} %{GREEDYDATA:message2}\) +%{WORD:was_errorcode} +%{SPACE}%{GREEDYDATA:message}"]
            match => ["message", "\[%{DATA:timestamp} %{WORD:tz}\] %{BASE16NUM:was_threadID} %{WORD:was_shortname} %{SPACE}%{WORD:was_loglevel} %{SPACE}%{GREEDYDATA:message}"]
            overwrite => [ "message" ]
        }
        date{
            match => ["timestamp", "dd/MM/YY HH:mm:ss:SSS", "M/d/YY HH:mm:ss:SSS", "MM/d/YY HH:mm:ss:SSS", "M/dd/YY HH:mm:ss:SSS", "MM/dd/YY H:mm:ss:SSS", "M/d/YY H:mm:ss:SSS", "MM/d/YY H:mm:ss:SSS", "M/dd/YY H:mm:ss:SSS"]
        }

        # Get Geographical IP information attached to event
        # geoip {
        #     source => "clientip"
        # }

        # If we get event with transaction id attached, store that id unitl new request come

        if [transaction_id] {
            ruby  {
                init => "@@map = {}"
                code => "@@map['transaction_id'] = event.get('transaction_id').to_s, @@map['was_threadID'] = event.get('was_threadID').to_s"
                # add_field => {"[@metadata][beat][transaction_id]" => "%{transaction_id}" }
            }
        } else {
            ruby {
                code => "if event.get('was_threadID') == @@map['was_threadID']
                            event.set('transaction_id',@@map['transaction_id'][0]) 
                        end"
                
            }
        }
    }

}

output {
  # Print each event to stdout, useful for debugging. Should be commented out in production.
  # Enabling 'rubydebug' codec on the stdout output will  make logstash
  # pretty-print the entire event as something similar to a JSON representation.
  #stdout {
   # codec => rubydebug
 # }
	# if "_grokparsefailure" not in [tags] {
  # Sending properly parsed log events to elasticsearch
  elasticsearch {
    hosts => ["localhost:9200"]
	manage_template => false
    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
	document_type => "%{[@metadata][type]}"
#   }
 }
}